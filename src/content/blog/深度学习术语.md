---
title: '深度学习术语'
description: '记录下防止自己忘记'
pubDate: '2025-2-5'
pinned: false
heroImage: '../../assets/cover.svg'
category: 'coding'
series: ''
tags: ['DL', "速查"]
---

| 术语 | 英文 | 大白话解释 |
| --- | --- | --- |
| 神经网络 | Neural Network | 一个可以通过「调参数」来学会做任务的程序 |
| 层 (Layer) | Layer | 网络里的一"排"处理单元，数据从一层流到下一层 |
| 全连接层 / 线性层 | Linear / Dense | 每个输入和每个输出之间都有连接（就是 $y=wx+b$） |
| 权重 | Weight (w) | 控制输入信号「放大多少倍」的参数 |
| 偏置 | Bias (b) | 控制输出「整体往上或往下移」的参数 |
| 激活函数 | Activation | 给网络加「转弯能力」的函数，没有它只能拟合直线 |
| ReLU | ReLU | 最常用的激活函数：负数变0，正数不变 |
| 前向传播 | Forward Pass | 把数据从输入「正着」传到输出，得到预测结果 |
| 损失 / 误差 | Loss | 预测值和真实值的差距有多大（越小越好） |
| 反向传播 | Backpropagation | 根据误差，倒着算出每个参数该怎么调 |
| 梯度 | Gradient | 告诉你「往哪个方向调参数，误差会变小」 |
| 学习率 | Learning Rate (lr) | 每次调参数的「步子大小」，太大会飞过头，太小训练太慢 |
| 优化器 | Optimizer | 负责拿着梯度去实际调整参数的"工人" |
| SGD | SGD | 最朴素的优化器：每次固定步子往梯度反方向走 |
| Adam | Adam | 更聪明的优化器：会根据历史调整步子大小和方向 |
| Epoch | Epoch | 把所有训练数据「完整过一遍」叫一个 epoch |
| 训练 | Training | 反复「预测→算误差→调参数」的过程 |
| 推理 / 预测 | Inference | 训练完了，用模型给新数据做预测 |
| eval 模式 | model.eval() | 告诉模型"我现在要做预测了，别学习了" |
| no_grad | torch.no_grad() | 告诉 PyTorch "别记梯度了，省点内存" |